---
title: "Taller 1 BDML"
author: Zeneth Olivero Tapia, Cristian Felipe Muñoz Guerrero, Laura Daniela Torres
  Diaz, Vivian Cabanzo Fernandez
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

# 1 Intriducción

La investigación sobre la distribución de los ingresos derivados del trabajo y factores que conllevan a diferencias salariales han sido un foco significativo en los estudios económicos y políticos.
Entender estos factores de edad, los estudios, el tiempo trabajado, la ocupación y el género en los ingresos permite no solo explicar las desigualdades en el mercado laboral, sino también buscar formas más justas y eficientes en la asignación de recursos.
Por eso, esta presente investigación busca analizar cómo cambia el ingreso en edad – salario, la brecha salarial de género y la ejecución de diversos modelos de predicción de ingresos en Bogotá, haciendo uso de datos sobre la población ocupada.

La fuente de información tomada es la Gran Encuesta Integrada de Hogares (GEIH) 2018, elaborada por el DANE, elaboramos una muestra de personas que viven en Bogotá.
Es una encuesta adecuada para el propósito del análisis, ya que incluye datos claves como el ingreso laboral, caracterización, demográficas y condiciones laborales.
Esto nos permite hacer uso de variables para estimaciones que reflejan las diferencias observables entre individuos como los patrones generales del mercado laboral.

En el primero componente, nos enfocamos en el análisis en el perfil edad – salario, estimando la relación entre la edad de los trabajadores y su ingreso por hora.
Se evidencia que los ingresos aumentan con la edad y la experiencia hasta alcanzar un punto máximo en la mediana edad (40 – 45 años de edad), seguido tiende a estabilizarse y/o decrecer, tal como lo indica la teoría del capital humano.
El segundo componente abordamos la brecha salarial de género, que siguen después de controlar por variables de educación y experiencia.
Aunque parte de la diferencia es por variables observables, y el análisis sugiere factores más profundos como una posible discriminación en diversos aspectos.

Por último, se evalúa que tan bien predicen distintos modelos econométricos.
Comparando la regresión lineal, polinomiales y otros métodos como LASSO y Random Forest.
Los resultados nos dicen que \_\_\_\_XXXXXX\_\_\_\_.

Investigar sobre como se distribuyen los ingresos que se generan del trabajo y que factores contribuyen a las diferencias salariales ha sido un tema importante en los estudios económicos y políticos.
 
## Configuración inicial

El primer bloque de código se utiliza para preparar el entorno de trabajo antes de iniciar el análisis de los datos.

1.  **Configuración de `knitr`**

2.  **Gestión de librerías con `pacman`**\
    Se verifica si el paquete `pacman` está instalado y, en caso contrario, se instala.

3.  **Carga de librerías para el análisis**\
    Cargue de librerías necesarias

```{r setup, include=FALSE}

# Configuración de knitr y carga de paquetes para análisis, limpieza y manipulación de datos

knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	fig.height = 6,
	fig.width = 10,
	message = FALSE,
	warning = TRUE, 
	tidy = TRUE,               
  tidy.opts = list(width.cutoff = 60)  
)

# Instalar y cargar pacman (gestor de paquetes) y otras librerías para análisis de datos

if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(rvest, #Web scrapping
               dplyr, #Manipulación de datos
               tidyr, #Manipúlación de datos
               readr, #Lectura de archivos csv
               janitor, #Limpieza rapida 
               purrr, #Programación funcional
               skimr, #Resumén descriptivo
               tidyverse, #Manipulación de datos
               styler, #Estilo de código
               corrplot, #Matrices de correlación
               boot, #Bootstrap
               modelsummary, #Resuménes en tablas
               scales,  # Para formatear ejes
               gt, #Mejores tablas
               broom)  
```

# 2. Datos

### Web scraping

En este bloque se realiza la **extracción automática de los datos** de la Gran Encuesta Integrada de Hogares (GEIH) 2018 para Bogotá, disponible en el sitio web del profesor Ignacio Sarmiento:
<https://ignaciomsarmiento.github.io/GEIH2018_sample/>.

1.  **Generación de URLs de las paginaciones**
    Los datos se encuentran distribuidos en **10 páginas HTML**.
    Se construye un vector de URLs concatenando la dirección base con los números del 1 al 10.

2.  **Función de extracción de tablas**
    Se define la función `funcion_para_leer_tablas()` que:

    -   Lee el contenido HTML de cada página con `read_html()`.
    -   Selecciona la primera tabla encontrada en la página (`html_elements(...)[[1]]`).
    -   Convierte dicha tabla en un `data.frame` con `html_table()`.
    -   Limpia los nombres de las variables y ajusta los tipos de datos con `clean_names()` y `type_convert()`.

3.  **Unificación de los datos en una sola base**
    Se aplica la función a todas las URLs con `map_dfr()`, lo que permite **leer y combinar las 10 tablas en un único `data.frame` (`data_completa`)**.

```{r cars}

# Generamos URL: contiene un vector de 10 paginaciones
urls <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", 1:10, ".html")

funcion_para_leer_tablas <- function(a) {
  pagina <- read_html(a)  #Leer el html
  tabla_html <- html_elements(pagina, "table")[[1]] #Extrae la primera página
  data <- html_table(tabla_html, trim = TRUE)  #Convertir el html a data frame
  data <- data %>% clean_names() %>% type_convert() #Limpiar nombres y cambiar tipos de datos
  return(data)
}

data_completa <- map_dfr(urls, funcion_para_leer_tablas) #Aplica lo anterior para todas las 10 paginaciones 

```

## Manipulación y limpieza de datos:

Los siguientes bloque de código tiene como propósito preparar la base de datos para el análisis incluyendo los siguientes pasos:

## Escogencia muestral

En este bloque se realiza una exploración preliminar de la base `data_completa` y la escogencia muestra:

1.  **Revisión de la estructura de los datos**
    -   Se utiliza `str(data_completa)` para identificar la estructura general de la base, encontrando que existen múltiples valores faltantes (NA) y que los tipos de datos más comunes son `character`, `integer` y `numeric`.\
    -   Se listan los nombres de todas las variables con `names(data_completa)`.\
    -   Se visualizan las primeras (`head()`) y últimas (`tail()`) observaciones para inspeccionar rápidamente el contenido.
2.  **Selección de variables de interés**
    -   Se identifican como relevantes las variables `age` (edad) y `ocu` (condición de ocupación).
3.  **Filtrado de la muestra**
    -   Se excluyen observaciones de individuos **menores de 18 años**.\
    -   Se conservan únicamente aquellos que reportan estar **ocupados (`ocu == 1`)**.
4.  **Revisión del tamaño muestral filtrado**
    -   Tras aplicar los filtros, la base resultante (`data_filter`) contiene **16.542 observaciones**.

```{r}
## Revisión inicial: 
# str para revisar la estructura de los datos
str(data_completa) #Vemos que hay una gran cantidad de NA y que los tipos mas comunes de datos son character, integrer, y numeral
# Names para identificar los nombres de todas las variables
names(data_completa) 
# head para ver las primeras observaciones
head(data_completa)
# tail para ver las ultimas   
tail(data_completa)

# Una vez encontrada las variables de interés para la selección de la muestra, 
# las cuales son age y ocu, procedemos a filtrar data_completa

## Filtramos observaciones inferiores a los 18 años de edad y no ocupados
data_filter <- data_completa%>%
  filter(age >= 18)%>%filter(ocu==1)

# Revisamos nuevamente la cantidad de observaciones
nrow(data_filter) #16.542 observaciones

```

## elección de variables

En este bloque se depura la base de datos filtrada (`data_filter`) para trabajar únicamente con las variables relevantes y se crea la variable de interés

Se conservan únicamente las siguientes columnas de interés: - `directorio`, `secuencia_p`, `orden`: variables de identificación del individuo.
- `sex`: sexo.
- `age`: edad.
- `relab``: tipo de ocupacion.
- `mes`: mes en la que se realizó la encuesta.
- `formal`: formalidad del trabajo.
- `size_firm`: tamaño de la firma donde trabaja.
- `max_educ_level`: máximo nivel educativo alcanzado.
- `impa`: Ingreso monetario primera actividad antes de imputación.
- `hours_work_usual`: horas trabajadas usualmente por semana.
- `cuenta_propia`: Trabajador por cuenta propia

Creamos la variable dependiente de la siguiente manera: - **Númerador**: `impa` - **Denominador**: `hours_work_usual` \* 4.33 - Donde `4.33` corresponde al promedio de semanas en un mes (52 semanas / 12 meses).

```{r}
# Con el nombre de las variables ya revisadas y 
data_filter<- data_filter%>% select(directorio, secuencia_p, orden,sex, age, relab, mes,
                                        formal, size_firm, max_educ_level, impa, hours_work_usual, cuenta_propia)

#Creación variable dependiente
data_filter<- data_filter%>%
  mutate(ingreso_hora_1 = (impa/(hours_work_usual*4.33)))

```

## Revisión inicial de los datos

En esta sección se realizó una exploración preliminar de la base de datos con los siguientes objetivos:

1.  **Identificación de individuos únicos**
    -   Se construyó una llave de identificación (`id_individuo`) combinando variables como `directorio`, `secuencia_p` y `orden`.\
    -   Con esta llave se verificó cuántos individuos únicos había en la base, evitando duplicados accidentales.
2.  **Conteo de meses observados**
    -   Se revisó cuántos meses distintos aparecen en la información de la GEIH.\
    -   Esto permitió confirmar la cobertura temporal de los datos.
3.  **Seguimiento de individuos a lo largo del tiempo**
    -   Se verificó si un mismo individuo (`id_individuo`) aparece en varios meses.\
    -   De esta manera se buscó identificar casos de panel (individuos observados más de una vez) y diferenciarlos de observaciones transversales (solo un mes).

```{r}

#Revisamos cantidad de meses que hay en la base:
n_meses <- data_filter %>%
  summarise(total_meses = n_distinct(mes)) %>%
  pull(total_meses)  #12 meses 

# Creamos el ID único para cada persona según la documentación de la GEIH
data_filter<- data_filter %>%
  mutate(id_individuo = paste(directorio, secuencia_p, orden, sep = "_"))

# Quitar duplicados a nivel individuo-mes
df_unicos <- data_filter%>%
  distinct(id_individuo, mes)

# Contar en cuántos meses aparece cada individuo:
individuos_completos <- df_unicos %>%
  group_by(id_individuo) %>%
  summarise(n_meses_individuo = n_distinct(mes), .groups = "drop") %>%
  mutate(esta_en_todos = n_meses_individuo == n_meses) #Cada individuo está en solo un mes

# Número total de individuos únicos:
total_individuos <- n_distinct(data_filter$id_individuo) 

# Número de individuos que aparecen en todos los meses
n_en_todos <- sum(individuos_completos$esta_en_todos)

#Resultados a nivel individuo
list(
  total_individuos = total_individuos, #16542 individuos
  meses_totales = n_meses, #12 meses
  individuos_en_todos_los_meses = n_en_todos #Ningún individuo se repite en ningún mes
)

# Eliminamos directorio, secuencia_p, orden, mes (ya no las necesitamos)
# Manipulación de NA: 
data_filter <- data_filter %>%
  select(-secuencia_p,-directorio,-orden,-mes)

```

## Revisión valores faltantes y ceros en la variable de ing43wo

En este bloque de código se realiza un análisis exploratorio de los valores faltantes en la base de datos `data_filter`, siguiendo los pasos:

1.  **Identificación de valores faltantes**
    -   Se usa la función `skim()` para obtener un resumen de cada variable, extrayendo el número de valores faltantes (`n_missing`).
2.  **Cálculo del porcentaje de valores faltantes**
    -   Se obtiene el número total de observaciones (`Obs = nrow(data_filter)`).\
    -   Se calcula el porcentaje de valores faltantes para cada variable dividiendo `n_missing / Obs`.
3.  **Filtrado de variables relevantes**
    -   Se eliminan las variables que no presentan valores faltantes (`n_missing = 0`) para centrar el análisis solo en aquellas con datos ausentes.
4.  **Visualización gráfica**
    -   Se construye un gráfico de barras con `ggplot2` que muestra el **porcentaje de valores faltantes por variable**.\
    -   Se incluye:
        -   Ordenamiento de las variables de mayor a menor porcentaje de faltantes.\
        -   Etiquetas con el porcentaje exacto sobre cada barra.

```{r}
# Revisamos valores faltantes:
data_miss <- skim(data_filter) %>% select( skim_variable, n_missing)
# Calculamnos número de observaciones para despues calcular el porcentaje de NA
Obs <- nrow(data_filter) 
#Porcentaje de observaciones faltantes
data_miss<- data_miss %>% mutate(p_missing= n_missing/Obs)
#Eliminar los que no tienen datos faltantes
data_miss<- data_miss %>% filter(n_missing!= 0)

# Revisar los datos faltantes gráficamente:
ggplot(data_miss, aes(x = reorder(skim_variable, p_missing), y = p_missing)) +
  geom_col(fill = "skyblue", color = "white", width = 0.7) +
  coord_flip() +
  geom_text(aes(label = scales::percent(p_missing, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Porcentaje de valores faltantes por variable",
    x = "Variable",
    y = "Porcentaje de NA"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 8)
  )

# Revisión de ceros en la variable dependiente
# Contar observaciones con ingreso por hora  = 0
n_ceros <- sum(data_filter$ingreso_hora_1 == 0, na.rm = TRUE)

# Calcular el porcentaje de ceros respecto al total
p_ceros <- n_ceros / Obs


# Gráfico simple para visualizar ceros vs. positivos
data_filter %>%
  mutate(ingreso_cero = ifelse(ingreso_hora_1 == 0, "0", "Mayor a 0")) %>%
  count(ingreso_cero) %>%
  ggplot(aes(x = ingreso_cero, y = n, fill = ingreso_cero)) +
  geom_col(show.legend = FALSE, width = 0.6) +
  geom_text(aes(label = n), vjust = -0.5) +
  labs(
    title = "Distribución de ingresos iguales a 0 vs. mayores a 0",
    x = "Tipo de ingreso",
    y = "Observaciones"
  ) +
  theme_minimal(base_size = 12)

```

## Revisión de outliers (valores atípicos):

En este bloque se evaluó la presencia de **valores atípicos** en la variable `ingreso_hora`.

1.  **Cálculo de límites de referencia:**
    -   Se definieron los percentiles 1% y 99% como puntos de corte.
    -   Los valores por debajo del percentil 1% (`low`) y por encima del percentil 99% (`up`) fueron considerados potenciales outliers.
2.  **Visualización mediante boxplot:**
    -   Se construyó un boxplot de la variable `ingreso_hora`.
    -   Se añadieron dos líneas horizontales punteadas en color azul que marcan los límites de outliers (`low` y `up`).
    -   Los valores atípicos fueron resaltados en color rojo dentro del gráfico.

```{r}
# Definimos límites inferior y superior de outliers usando percentiles
low <- quantile(data_filter$ingreso_hora_1, 0.01, na.rm = TRUE)   # Percentil 1%
up  <- quantile(data_filter$ingreso_hora_1, 0.99, na.rm = TRUE)   # Percentil 99%

# Mostramos los valores de referencia
low
up

# Creamos boxplot para visualizar distribución de ingreso por hora (en log)
plot_outlier <- ggplot(data = data_filter, aes(x = "", y = ingreso_hora_1)) +
  geom_boxplot(fill = "skyblue", outlier.color = "red", outlier.size = 1) + # boxplot con outliers resaltados
  geom_hline(yintercept = low, color = "blue", linetype = "dashed", linewidth = 0.7) + # línea límite inferior
  geom_hline(yintercept = up,  color = "blue", linetype = "dashed", linewidth = 0.7) + # línea límite superior
  labs(
    title = "Boxplot de salario horario con límites de outliers",
    y = "Log(Ingreso por hora)",
    x = ""
  ) +
  theme_minimal()

# Mostrar el gráfico
print(plot_outlier)

```

## Manipulación final de datos

En esta etapa se realizaron dos procesos fundamentales:

1.  **transformación de variables categóricas:**\
    Se identificaron aquellas variables que originalmente estaban codificadas con números enteros pero que representan categorías. Estas fueron convertidas a factores y se les asignaron etiquetas descriptivas para facilitar su interpretación.
    -   `sex` → `sexo`: recodificada como **Mujer** y **Hombre**.\
    -   `formal`: clasificada en **Informal** y **Formal**.
    -   `oficio`: transformada en factor.
    -   `size_firm`: recodificada en categorías que representan el **tamaño de la empresa** (auto-empleado, 2–5 empleados, 6–10 empleados, 11–15 empleados y más de 50).\
    -   `max_educ_level`: recodificada a niveles educativos alcanzados (ninguno, preescolar, primaria, secundaria, terciaria, N/A).\
    -   `cuenta_propia`: categorizada en **Cuenta propia** y **No cuenta propia**.
2.  **Tratamiento de la variable de ingresos:**
    -   Se eliminan las observaciones con **ingreso por hora igual a cero**, dado que no son válidas para el análisis de logaritmos y, además, podrían distorsionar la interpretación del salario.\
    -   Se crea la nueva variable `log_ingreso_hora`, que corresponde al **logaritmo natural del ingreso por hora**. Esta transformación ayuda a reducir la influencia de valores atípicos (outliers) y aproxima la distribución de la variable a una forma más cercana a la normalidad.

```{r}
# Revisamos nuevamente la estructura de los datos
str(data_filter)

# Cambiamos a factor las variables que son categóricas
data_filter <- data_filter %>%
  mutate(
    sexo = factor(sex, 
                 levels = c(0, 1), 
                 labels = c("Mujer", "Hombre")),
    formal = factor(formal, 
                    levels = c(0, 1), 
                    labels = c("Informal", "Formal")),
    relab = factor(relab,
                   levels = c(1,2,3,4,5,6,7,8,9),
                   labels = c("Empleado empresa",
                               "Empleado del gobierno",
                               "Empleado doméstico", 
                               "Trabajador cuenta propia",
                               "Empleador",
                               "Trabajador familiar sin remuneración",
                               "Trabajador sin remuneración otras empresas",
                               "Jornalero",
                               "Otro")),
    size_firm = factor(size_firm,
                       levels = c(1, 2, 3, 4, 5),
                       labels = c("auto-empleado", 
                                  "2-5 empleados", 
                                  "6-510 empleados", 
                                  "11-15 empleados", 
                                  "Más de 50")),
    max_educ_level = factor(max_educ_level,
                            levels = c(1, 2, 3, 4, 5, 6, 7,9),
                            labels = c("Ninguno", "Preescolar", "Primaria imcompleta", 
                                       "Primaria completa", "Secundaria incompleta", "Secundaria completa", "Terceiaria", "N/A")),
    cuenta_propia = factor(cuenta_propia, 
                           levels = c(0, 1), 
                           labels = c("No cuenta propia", "Cuenta propia"))
  )

# Manipulación de la variable ingreso por hora para reducción de peso de outliers
data_filter <- data_filter %>%
  filter(ingreso_hora_1> 0) %>%  # elimina ceros porque logaritmo no permite tener 0
  mutate(log_ingreso_hora = log(ingreso_hora_1))

```

## Estadísticas descriptivas de las variables

```{r}

# Tabla descriptiva general
# Seleccionar solo las variables deseadas
vars <- data_filter[, c("ingreso_hora_1", "age", "hours_work_usual", "max_educ_level", "sex")]

# Generar resumen
datasummary_skim(vars, output = "gt", histogram = FALSE)

# Tabla descriptiva por sexo
datos <- data_filter %>%
  mutate(sex_label = ifelse(sex == 0, "Mujer", "Hombre"))

datasummary_balance(
  ingreso_hora_1 + age + hours_work_usual + max_educ_level ~ sex_label,
  data = datos,
  fmt = 2
)
# Gráfico: densidad de ln(w) por sexo


ggplot(datos, aes(x = log(ingreso_hora_1), fill = sex_label)) +
  geom_density(alpha = 0.6, size = 1.1, color = NA) +
  scale_fill_manual(values = c("Mujer" = "#F8BBD0", "Hombre" = "#1E88E5")) +
  labs(
    title = "Distribución del logaritmo del salario horario por sexo",
    subtitle = "Comparación entre hombres y mujeres",
    x = "Logaritmo del ingreso por hora",
    y = "Densidad",
    fill = "Sexo"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
    legend.position = "top",
    legend.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

```

```{r}
ggplot(data_filter, aes(x = max_educ_level, y = log(ingreso_hora_1))) +
  geom_boxplot(fill = "#81D4FA") +
  labs(
    title = "Salario horario por nivel educativo",
    subtitle = "Comparación entre trabajadores formales e informales",
    x = "Nivel educativo",
    y = "Logaritmo del ingreso por hora"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
    legend.position = "top",
    legend.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

```

```{r}

# Calcular proporciones
porcentajes <- data_filter %>%
  count(formal) %>%
  mutate(
    prop = n / sum(n),
    label = paste0(round(prop * 100, 1), "%"),
    formal = factor(formal, levels = c(0, 1), labels = c("No formal", "Formal"))
  )

# Gráfico
ggplot(data_filter, aes(x = factor(formal, levels = c(0, 1), labels = c("No formal", "Formal")))) +
  geom_bar(fill = "#4682B4", width = 0.6) +
  geom_text(
    data = porcentajes,
    aes(x = formal, y = n, label = label),
    vjust = -0.4,
    color = "gray20"
  ) +
  labs(
    title = "Distribución de la formalidad laboral",
    subtitle = "Comparación entre trabajadores formales e informales",
    x = "Condición laboral",
    y = "Frecuencia"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
    axis.title = element_text(face = "bold"),
    axis.text = element_text(color = "gray20"),
    panel.grid.minor = element_blank()
  )

```

## 4.Brecha Salarial de género

# 4.a Brecha incondicional

$$
\ln(w) = \beta_1 + \beta_2 \cdot \text{sex} + u
$$

```{r}
datos_lim<-data_filter

wage_gap <- lm(log_ingreso_hora ~ sexo, data = datos_lim)

#Visualización de tabla

tidy(wage_gap, conf.int = TRUE) %>%
  mutate(term = recode(term, "(Intercept)" = "Intercepto", "sexo" = "Sexo (1 = hombres)")) %>%
  gt() %>%
  tab_header(
    title = "Estimación de Brecha Salarial",
    subtitle = "Modelo log-lineal sin controles"
  ) %>%
  fmt_number(columns = c(estimate, std.error, statistic, p.value, conf.low, conf.high), decimals = 2)


# Interpretación en porcentaje
gap_incond <- 100*(exp(coef(wage_gap)["sexo"]) - 1)
gap_incond


```

# 4.b Brecha condicional con controles

$$
\ln(w) = \beta_1 + \beta_2 \cdot \text{sex} + \theta X + u
$$

donde:

-   $w$: salario\
-   $\text{sex}$: variable binaria de sexo\
-   $X = \{ \text{edad}, \text{edad}^2, \text{educación}, \text{oficio}, \text{formalidad}, \text{tamaño de empresa} \}$\
-   $u$: término de error aleatorio

```{r}
# Filtro de datos
gap_cond <- lm(log_ingreso_hora~ sexo + age + I(age^2) + max_educ_level +
               relab + formal + size_firm, 
             data = datos_lim)
#Visualización de tabla
tidy(gap_cond, conf.int = TRUE) %>%
  mutate(term = recode(term, "(Intercept)" = "Intercepto", "sexo" = "Sexo (1 = hombres)")) %>%
  gt() %>%
  tab_header(
    title = "Estimación de Brecha Condicional con Controles",
    subtitle = "Modelo log-lineal con controles"
  ) %>%
  fmt_number(columns = c(estimate, std.error, statistic, p.value, conf.low, conf.high), decimals = 2)

# Interpretación en porcentaje
gap_cond <- 100*(exp(coef(gap_cond)["sex"]) - 1)
gap_cond

```

# 4.b.i FWL (Frisch-Waugh-Lovell)

```{r}
# Paso 1: sex ~ X
res_sex <- lm(sex ~ age + I(age^2) + max_educ_level +
                 relab + formal + size_firm , 
              data = datos_lim)$residuals

# Paso 2: ln(w) ~ X
res_w <- lm(log_ingreso_hora ~ age + I(age^2) + max_educ_level +
              relab + formal + size_firm , 
            data = datos_lim)$residuals

# Paso 3: residuales
m_fwl <- lm(res_w ~ res_sex)
summary(m_fwl)

#Visualización de tabla
tidy(m_fwl, conf.int = TRUE) %>%
  mutate(term = recode(term, "(Intercept)" = "Intercepto", "sex" = "Sexo (1 = hombres)")) %>%
  gt() %>%
  tab_header(
    title = "Estimación de Brecha Condicional con Controles",
    subtitle = "Modelo log-lineal con controles"
  ) %>%
  fmt_number(columns = c(estimate, std.error, statistic, p.value, conf.low, conf.high), decimals = 2)

# Interpretación en porcentaje
gap_fwl <- 100*(exp(coef(m_fwl)["res_sex"]) - 1)
gap_fwl  


```

# 4.b.ii FWL con bootstrap

```{r}


boot_fwl <- function(data, index){
  d <- data[index, ]
  r_sex <- lm(sex ~ age + I(age^2) + max_educ_level +
                 relab + formal + size_firm, 
              data = d)$residuals
  r_w   <- lm(log_ingreso_hora ~ age + I(age^2) + max_educ_level +
                 relab + formal + size_firm , 
              data = d)$residuals
  coef(lm(r_w ~ r_sex))[2]
}
 
set.seed(123)
boot_res <- boot(data = datos_lim, statistic = boot_fwl, R = 500)
boot.ci(boot_res, type="perc")

# Estimador FWL clásico
beta_fwl <- coef(m_fwl)["res_sex"]
se_fwl   <- summary(m_fwl)$coefficients["res_sex", "Std. Error"]

# Estimador FWL bootstrap
beta_boot <- mean(boot_res$t)
se_boot   <- sd(boot_res$t)

results <- data.frame(
  Metodo = c("FWL clásico", "FWL bootstrap"),
  Estimador = c(beta_fwl, beta_boot),
  SE = c(se_fwl, se_boot)
)
results

```
